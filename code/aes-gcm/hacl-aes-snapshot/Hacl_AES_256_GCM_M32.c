/* 
  This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
  KreMLin invocation: /Users/bhargava/Desktop/repositories/kremlin//krml -funroll-loops 8 -warn-error +9 -I /Users/bhargava/Desktop/repositories/hacl-star-dev//lib -I /Users/bhargava/Desktop/repositories/kremlin//kremlib -I /Users/bhargava/Desktop/repositories/hacl-star-dev//specs -I . -I /Users/bhargava/Desktop/repositories/hacl-star-dev//code/experimental/aes -I /Users/bhargava/Desktop/repositories/hacl-star-dev//code/experimental/gf128 -ccopt -march=native -fbuiltin-uint128 -drop FStar.UInt128 -fnocompound-literals -fparentheses -fcurly-braces -tmpdir aesgcm-aes256-m32-c aesgcm-aes256-m32-c/out.krml -skip-compilation -minimal -add-include "kremlib.h" -drop Lib.Vec128 -bundle Hacl.AES_256_GCM.M32=*
  F* version: 7c70b890
  KreMLin version: b511d90c
 */

#include "Hacl_AES_256_GCM_M32.h"

static uint64_t Hacl_Spec_AES_128_BitSlice_transpose_bits64(uint64_t x)
{
  return
    ((((((((((((((x & (uint64_t)0x8040201008040201U)
    | (x & (uint64_t)0x4020100804020100U) >> (uint32_t)7U)
    | (x & (uint64_t)0x2010080402010000U) >> (uint32_t)14U)
    | (x & (uint64_t)0x1008040201000000U) >> (uint32_t)21U)
    | (x & (uint64_t)0x0804020100000000U) >> (uint32_t)28U)
    | (x & (uint64_t)0x0402010000000000U) >> (uint32_t)35U)
    | (x & (uint64_t)0x0201000000000000U) >> (uint32_t)42U)
    | (x & (uint64_t)0x0100000000000000U) >> (uint32_t)49U)
    | (x << (uint32_t)7U & (uint64_t)0x4020100804020100U))
    | (x << (uint32_t)14U & (uint64_t)0x2010080402010000U))
    | (x << (uint32_t)21U & (uint64_t)0x1008040201000000U))
    | (x << (uint32_t)28U & (uint64_t)0x0804020100000000U))
    | (x << (uint32_t)35U & (uint64_t)0x0402010000000000U))
    | (x << (uint32_t)42U & (uint64_t)0x0201000000000000U))
    | (x << (uint32_t)49U & (uint64_t)0x0100000000000000U);
}

static void Hacl_Impl_AES_CoreBitSlice_load_block0(uint64_t *out, uint8_t *inp)
{
  uint8_t *b1 = inp;
  uint8_t *b2 = inp + (uint32_t)8U;
  uint64_t u0 = load64_le(b1);
  uint64_t fst1 = u0;
  uint64_t u1 = load64_le(b2);
  uint64_t snd1 = u1;
  uint64_t fst2 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(fst1);
  uint64_t snd2 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(snd1);
  {
    uint32_t sh = (uint32_t)0U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[0U] = u2;
  }
  {
    uint32_t sh = (uint32_t)1U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[1U] = u2;
  }
  {
    uint32_t sh = (uint32_t)2U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[2U] = u2;
  }
  {
    uint32_t sh = (uint32_t)3U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[3U] = u2;
  }
  {
    uint32_t sh = (uint32_t)4U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[4U] = u2;
  }
  {
    uint32_t sh = (uint32_t)5U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[5U] = u2;
  }
  {
    uint32_t sh = (uint32_t)6U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[6U] = u2;
  }
  {
    uint32_t sh = (uint32_t)7U * (uint32_t)8U;
    uint64_t u = fst2 >> sh & (uint64_t)0xffU;
    uint64_t u2 = u ^ (snd2 >> sh & (uint64_t)0xffU) << (uint32_t)8U;
    out[7U] = u2;
  }
}

static void Hacl_Impl_AES_CoreBitSlice_transpose_state(uint64_t *st)
{
  uint64_t i0 = st[0U];
  uint64_t i1 = st[1U];
  uint64_t i2 = st[2U];
  uint64_t i3 = st[3U];
  uint64_t i4 = st[4U];
  uint64_t i5 = st[5U];
  uint64_t i6 = st[6U];
  uint64_t i7 = st[7U];
  uint64_t t00 = (i0 & (uint64_t)0xffffffffU) ^ i4 << (uint32_t)32U;
  uint64_t t10 = (i1 & (uint64_t)0xffffffffU) ^ i5 << (uint32_t)32U;
  uint64_t t20 = (i2 & (uint64_t)0xffffffffU) ^ i6 << (uint32_t)32U;
  uint64_t t30 = (i3 & (uint64_t)0xffffffffU) ^ i7 << (uint32_t)32U;
  uint64_t t40 = (i4 & (uint64_t)0xffffffff00000000U) ^ i0 >> (uint32_t)32U;
  uint64_t t50 = (i5 & (uint64_t)0xffffffff00000000U) ^ i1 >> (uint32_t)32U;
  uint64_t t60 = (i6 & (uint64_t)0xffffffff00000000U) ^ i2 >> (uint32_t)32U;
  uint64_t t70 = (i7 & (uint64_t)0xffffffff00000000U) ^ i3 >> (uint32_t)32U;
  uint64_t t0_ = t00;
  uint64_t t1_ = t10;
  uint64_t t4_ = t40;
  uint64_t t5_ = t50;
  uint64_t
  t01 =
    (t00 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t20 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t11 =
    (t10 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t30 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t21 =
    (t20 & (uint64_t)0xffff0000ffff0000U)
    ^ (t0_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t
  t31 =
    (t30 & (uint64_t)0xffff0000ffff0000U)
    ^ (t1_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t
  t41 =
    (t40 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t60 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t51 =
    (t50 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t70 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t61 =
    (t60 & (uint64_t)0xffff0000ffff0000U)
    ^ (t4_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t
  t71 =
    (t70 & (uint64_t)0xffff0000ffff0000U)
    ^ (t5_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t t0_1 = t01;
  uint64_t t2_1 = t21;
  uint64_t t4_1 = t41;
  uint64_t t6_1 = t61;
  uint64_t
  t02 =
    (t01 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t11 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t12 =
    (t11 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t0_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t
  t22 =
    (t21 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t31 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t32 =
    (t31 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t2_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t
  t42 =
    (t41 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t51 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t52 =
    (t51 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t4_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t
  t62 =
    (t61 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t71 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t72 =
    (t71 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t6_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t t03 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t02);
  uint64_t t13 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t12);
  uint64_t t23 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t22);
  uint64_t t33 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t32);
  uint64_t t43 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t42);
  uint64_t t53 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t52);
  uint64_t t63 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t62);
  uint64_t t73 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t72);
  uint64_t t0 = t03;
  uint64_t t1 = t13;
  uint64_t t2 = t23;
  uint64_t t3 = t33;
  uint64_t t4 = t43;
  uint64_t t5 = t53;
  uint64_t t6 = t63;
  uint64_t t7 = t73;
  st[0U] = t0;
  st[1U] = t1;
  st[2U] = t2;
  st[3U] = t3;
  st[4U] = t4;
  st[5U] = t5;
  st[6U] = t6;
  st[7U] = t7;
}

static void Hacl_Impl_AES_CoreBitSlice_store_block0(uint8_t *out, uint64_t *inp)
{
  uint64_t i0 = inp[0U];
  uint64_t i1 = inp[1U];
  uint64_t i2 = inp[2U];
  uint64_t i3 = inp[3U];
  uint64_t i4 = inp[4U];
  uint64_t i5 = inp[5U];
  uint64_t i6 = inp[6U];
  uint64_t i7 = inp[7U];
  uint64_t t0 = (i0 & (uint64_t)0xffffffffU) ^ i4 << (uint32_t)32U;
  uint64_t t10 = (i1 & (uint64_t)0xffffffffU) ^ i5 << (uint32_t)32U;
  uint64_t t2 = (i2 & (uint64_t)0xffffffffU) ^ i6 << (uint32_t)32U;
  uint64_t t3 = (i3 & (uint64_t)0xffffffffU) ^ i7 << (uint32_t)32U;
  uint64_t t4 = (i4 & (uint64_t)0xffffffff00000000U) ^ i0 >> (uint32_t)32U;
  uint64_t t5 = (i5 & (uint64_t)0xffffffff00000000U) ^ i1 >> (uint32_t)32U;
  uint64_t t6 = (i6 & (uint64_t)0xffffffff00000000U) ^ i2 >> (uint32_t)32U;
  uint64_t t7 = (i7 & (uint64_t)0xffffffff00000000U) ^ i3 >> (uint32_t)32U;
  uint64_t t0_ = t0;
  uint64_t t1_ = t10;
  uint64_t t4_ = t4;
  uint64_t t5_ = t5;
  uint64_t
  t01 =
    (t0 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t2 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t11 =
    (t10 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t3 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t21 =
    (t2 & (uint64_t)0xffff0000ffff0000U)
    ^ (t0_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t
  t31 =
    (t3 & (uint64_t)0xffff0000ffff0000U)
    ^ (t1_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t
  t41 =
    (t4 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t6 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t51 =
    (t5 & (uint64_t)0x0000ffff0000ffffU)
    ^ (t7 & (uint64_t)0x0000ffff0000ffffU) << (uint32_t)16U;
  uint64_t
  t61 =
    (t6 & (uint64_t)0xffff0000ffff0000U)
    ^ (t4_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t
  t71 =
    (t7 & (uint64_t)0xffff0000ffff0000U)
    ^ (t5_ & (uint64_t)0xffff0000ffff0000U) >> (uint32_t)16U;
  uint64_t t0_1 = t01;
  uint64_t t2_1 = t21;
  uint64_t t4_1 = t41;
  uint64_t t6_1 = t61;
  uint64_t
  t02 =
    (t01 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t11 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t12 =
    (t11 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t0_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t
  t22 =
    (t21 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t31 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t32 =
    (t31 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t2_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t
  t42 =
    (t41 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t51 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t52 =
    (t51 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t4_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t
  t62 =
    (t61 & (uint64_t)0x00ff00ff00ff00ffU)
    ^ (t71 & (uint64_t)0x00ff00ff00ff00ffU) << (uint32_t)8U;
  uint64_t
  t72 =
    (t71 & (uint64_t)0xff00ff00ff00ff00U)
    ^ (t6_1 & (uint64_t)0xff00ff00ff00ff00U) >> (uint32_t)8U;
  uint64_t t03 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t02);
  uint64_t t13 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t12);
  uint64_t t23 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t22);
  uint64_t t33 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t32);
  uint64_t t43 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t42);
  uint64_t t53 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t52);
  uint64_t t63 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t62);
  uint64_t t73 = Hacl_Spec_AES_128_BitSlice_transpose_bits64(t72);
  uint64_t t00 = t03;
  uint64_t t1 = t13;
  store64_le(out, t00);
  store64_le(out + (uint32_t)8U, t1);
}

static void Hacl_Impl_AES_CoreBitSlice_load_key1(uint64_t *out, uint8_t *k)
{
  Hacl_Impl_AES_CoreBitSlice_load_block0(out, k);
  {
    uint64_t u = out[0U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[0U] = u3;
  }
  {
    uint64_t u = out[1U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[1U] = u3;
  }
  {
    uint64_t u = out[2U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[2U] = u3;
  }
  {
    uint64_t u = out[3U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[3U] = u3;
  }
  {
    uint64_t u = out[4U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[4U] = u3;
  }
  {
    uint64_t u = out[5U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[5U] = u3;
  }
  {
    uint64_t u = out[6U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[6U] = u3;
  }
  {
    uint64_t u = out[7U];
    uint64_t u2 = u ^ u << (uint32_t)16U;
    uint64_t u3 = u2 ^ u2 << (uint32_t)32U;
    out[7U] = u3;
  }
}

static void Hacl_Impl_AES_CoreBitSlice_load_nonce(uint64_t *out, uint8_t *nonce)
{
  uint8_t nb[16U] = { 0U };
  memcpy(nb, nonce, (uint32_t)12U * sizeof nonce[0U]);
  Hacl_Impl_AES_CoreBitSlice_load_key1(out, nb);
}

static void
Hacl_Impl_AES_CoreBitSlice_load_state(uint64_t *out, uint64_t *nonce, uint32_t counter)
{
  uint8_t ctr[16U] = { 0U };
  store32_be(ctr, counter);
  store32_be(ctr + (uint32_t)4U, counter + (uint32_t)1U);
  store32_be(ctr + (uint32_t)8U, counter + (uint32_t)2U);
  store32_be(ctr + (uint32_t)12U, counter + (uint32_t)3U);
  Hacl_Impl_AES_CoreBitSlice_load_block0(out, ctr);
  {
    uint64_t u = out[0U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[0U] = u3 ^ nonce[0U];
  }
  {
    uint64_t u = out[1U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[1U] = u3 ^ nonce[1U];
  }
  {
    uint64_t u = out[2U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[2U] = u3 ^ nonce[2U];
  }
  {
    uint64_t u = out[3U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[3U] = u3 ^ nonce[3U];
  }
  {
    uint64_t u = out[4U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[4U] = u3 ^ nonce[4U];
  }
  {
    uint64_t u = out[5U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[5U] = u3 ^ nonce[5U];
  }
  {
    uint64_t u = out[6U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[6U] = u3 ^ nonce[6U];
  }
  {
    uint64_t u = out[7U];
    uint64_t
    u2 = ((u << (uint32_t)12U | u << (uint32_t)24U) | u << (uint32_t)36U) | u << (uint32_t)48U;
    uint64_t u3 = u2 & (uint64_t)0xf000f000f000f000U;
    out[7U] = u3 ^ nonce[7U];
  }
}

static void Hacl_Impl_AES_CoreBitSlice_xor_state_key1(uint64_t *st, uint64_t *ost)
{
  {
    st[0U] = st[0U] ^ ost[0U];
  }
  {
    st[1U] = st[1U] ^ ost[1U];
  }
  {
    st[2U] = st[2U] ^ ost[2U];
  }
  {
    st[3U] = st[3U] ^ ost[3U];
  }
  {
    st[4U] = st[4U] ^ ost[4U];
  }
  {
    st[5U] = st[5U] ^ ost[5U];
  }
  {
    st[6U] = st[6U] ^ ost[6U];
  }
  {
    st[7U] = st[7U] ^ ost[7U];
  }
}

static void Hacl_Impl_AES_CoreBitSlice_xor_block(uint8_t *out, uint64_t *st, uint8_t *inp)
{
  Hacl_Impl_AES_CoreBitSlice_transpose_state(st);
  {
    uint8_t *ob = out + (uint32_t)0U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)0U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[0U]);
  }
  {
    uint8_t *ob = out + (uint32_t)1U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)1U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[1U]);
  }
  {
    uint8_t *ob = out + (uint32_t)2U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)2U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[2U]);
  }
  {
    uint8_t *ob = out + (uint32_t)3U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)3U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[3U]);
  }
  {
    uint8_t *ob = out + (uint32_t)4U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)4U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[4U]);
  }
  {
    uint8_t *ob = out + (uint32_t)5U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)5U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[5U]);
  }
  {
    uint8_t *ob = out + (uint32_t)6U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)6U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[6U]);
  }
  {
    uint8_t *ob = out + (uint32_t)7U * (uint32_t)8U;
    uint8_t *ib = inp + (uint32_t)7U * (uint32_t)8U;
    uint64_t u = load64_le(ib);
    uint64_t u0 = u;
    store64_le(ob, u0 ^ st[7U]);
  }
}

static void Hacl_Impl_AES_CoreBitSlice_sub_bytes_state(uint64_t *st)
{
  uint64_t u0 = st[7U];
  uint64_t u11 = st[6U];
  uint64_t u2 = st[5U];
  uint64_t u3 = st[4U];
  uint64_t u4 = st[3U];
  uint64_t u5 = st[2U];
  uint64_t u6 = st[1U];
  uint64_t u7 = st[0U];
  uint64_t t1 = u6 ^ u4;
  uint64_t t2 = u3 ^ u0;
  uint64_t t3 = u11 ^ u2;
  uint64_t t6 = u11 ^ u5;
  uint64_t t7 = u0 ^ u6;
  uint64_t t13 = u2 ^ u5;
  uint64_t t16 = u0 ^ u5;
  uint64_t t18 = u6 ^ u5;
  uint64_t t4 = u7 ^ t3;
  uint64_t t5 = t1 ^ t2;
  uint64_t t8 = t1 ^ t6;
  uint64_t t9 = u6 ^ t4;
  uint64_t t10 = u3 ^ t4;
  uint64_t t11 = u7 ^ t5;
  uint64_t t12 = t5 ^ t6;
  uint64_t t14 = t3 ^ t5;
  uint64_t t15 = u5 ^ t7;
  uint64_t t17 = u7 ^ t8;
  uint64_t t19 = t2 ^ t18;
  uint64_t t22 = u0 ^ t4;
  uint64_t t54 = t2 & t8;
  uint64_t t50 = t9 & t4;
  uint64_t t20 = t4 ^ t15;
  uint64_t t21 = t1 ^ t13;
  uint64_t t39 = t21 ^ t5;
  uint64_t t40 = t21 ^ t7;
  uint64_t t41 = t7 ^ t19;
  uint64_t t42 = t16 ^ t14;
  uint64_t t43 = t22 ^ t17;
  uint64_t t44 = t19 & t5;
  uint64_t t45 = t20 & t11;
  uint64_t t47 = t10 & u7;
  uint64_t t57 = t16 & t14;
  uint64_t t46 = t12 ^ t44;
  uint64_t t48 = t47 ^ t44;
  uint64_t t49 = t7 & t21;
  uint64_t t51 = t40 ^ t49;
  uint64_t t52 = t22 & t17;
  uint64_t t53 = t52 ^ t49;
  uint64_t t55 = t41 & t39;
  uint64_t t56 = t55 ^ t54;
  uint64_t t58 = t57 ^ t54;
  uint64_t t59 = t46 ^ t45;
  uint64_t t60 = t48 ^ t42;
  uint64_t t61 = t51 ^ t50;
  uint64_t t62 = t53 ^ t58;
  uint64_t t63 = t59 ^ t56;
  uint64_t t64 = t60 ^ t58;
  uint64_t t65 = t61 ^ t56;
  uint64_t t66 = t62 ^ t43;
  uint64_t t67 = t65 ^ t66;
  uint64_t t68 = t65 & t63;
  uint64_t t69 = t64 ^ t68;
  uint64_t t70 = t63 ^ t64;
  uint64_t t71 = t66 ^ t68;
  uint64_t t72 = t71 & t70;
  uint64_t t73 = t69 & t67;
  uint64_t t74 = t63 & t66;
  uint64_t t75 = t70 & t74;
  uint64_t t76 = t70 ^ t68;
  uint64_t t77 = t64 & t65;
  uint64_t t78 = t67 & t77;
  uint64_t t79 = t67 ^ t68;
  uint64_t t80 = t64 ^ t72;
  uint64_t t81 = t75 ^ t76;
  uint64_t t82 = t66 ^ t73;
  uint64_t t83 = t78 ^ t79;
  uint64_t t84 = t81 ^ t83;
  uint64_t t85 = t80 ^ t82;
  uint64_t t86 = t80 ^ t81;
  uint64_t t87 = t82 ^ t83;
  uint64_t t88 = t85 ^ t84;
  uint64_t t89 = t87 & t5;
  uint64_t t90 = t83 & t11;
  uint64_t t91 = t82 & u7;
  uint64_t t92 = t86 & t21;
  uint64_t t93 = t81 & t4;
  uint64_t t94 = t80 & t17;
  uint64_t t95 = t85 & t8;
  uint64_t t96 = t88 & t39;
  uint64_t t97 = t84 & t14;
  uint64_t t98 = t87 & t19;
  uint64_t t99 = t83 & t20;
  uint64_t t100 = t82 & t10;
  uint64_t t101 = t86 & t7;
  uint64_t t102 = t81 & t9;
  uint64_t t103 = t80 & t22;
  uint64_t t104 = t85 & t2;
  uint64_t t105 = t88 & t41;
  uint64_t t106 = t84 & t16;
  uint64_t t107 = t104 ^ t105;
  uint64_t t108 = t93 ^ t99;
  uint64_t t109 = t96 ^ t107;
  uint64_t t110 = t98 ^ t108;
  uint64_t t111 = t91 ^ t101;
  uint64_t t112 = t89 ^ t92;
  uint64_t t113 = t107 ^ t112;
  uint64_t t114 = t90 ^ t110;
  uint64_t t115 = t89 ^ t95;
  uint64_t t116 = t94 ^ t102;
  uint64_t t117 = t97 ^ t103;
  uint64_t t118 = t91 ^ t114;
  uint64_t t119 = t111 ^ t117;
  uint64_t t120 = t100 ^ t108;
  uint64_t t121 = t92 ^ t95;
  uint64_t t122 = t110 ^ t121;
  uint64_t t123 = t106 ^ t119;
  uint64_t t124 = t104 ^ t115;
  uint64_t t125 = t111 ^ t116;
  uint64_t st70 = t109 ^ t122;
  uint64_t st50 = ~(t123 ^ t124);
  uint64_t t128 = t94 ^ t107;
  uint64_t st40 = t113 ^ t114;
  uint64_t st30 = t118 ^ t128;
  uint64_t t131 = t93 ^ t101;
  uint64_t t132 = t112 ^ t120;
  uint64_t st00 = ~(t113 ^ t125);
  uint64_t t134 = t97 ^ t116;
  uint64_t t135 = t131 ^ t134;
  uint64_t t136 = t93 ^ t115;
  uint64_t st10 = ~(t109 ^ t135);
  uint64_t t138 = t119 ^ t132;
  uint64_t st20 = t109 ^ t138;
  uint64_t t140 = t114 ^ t136;
  uint64_t st60 = ~(t109 ^ t140);
  uint64_t st0 = st00;
  uint64_t st1 = st10;
  uint64_t st2 = st20;
  uint64_t st3 = st30;
  uint64_t st4 = st40;
  uint64_t st5 = st50;
  uint64_t st6 = st60;
  uint64_t st7 = st70;
  st[0U] = st0;
  st[1U] = st1;
  st[2U] = st2;
  st[3U] = st3;
  st[4U] = st4;
  st[5U] = st5;
  st[6U] = st6;
  st[7U] = st7;
}

static void Hacl_Impl_AES_CoreBitSlice_shift_rows_state(uint64_t *st)
{
  {
    uint64_t rowi = st[0U];
    st[0U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[1U];
    st[1U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[2U];
    st[2U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[3U];
    st[3U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[4U];
    st[4U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[5U];
    st[5U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[6U];
    st[6U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
  {
    uint64_t rowi = st[7U];
    st[7U] =
      ((((((rowi & (uint64_t)0x1111111111111111U)
      | (rowi & (uint64_t)0x2220222022202220U) >> (uint32_t)4U)
      | (rowi & (uint64_t)0x0002000200020002U) << (uint32_t)12U)
      | (rowi & (uint64_t)0x4400440044004400U) >> (uint32_t)8U)
      | (rowi & (uint64_t)0x0044004400440044U) << (uint32_t)8U)
      | (rowi & (uint64_t)0x8000800080008000U) >> (uint32_t)12U)
      | (rowi & (uint64_t)0x0888088808880888U) << (uint32_t)4U;
  }
}

static void Hacl_Impl_AES_CoreBitSlice_mix_columns_state(uint64_t *st)
{
  uint64_t col[8U] = { 0U };
  {
    uint64_t coli = st[0U];
    col[0U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[1U];
    col[1U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[2U];
    col[2U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[3U];
    col[3U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[4U];
    col[4U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[5U];
    col[5U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[6U];
    col[6U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  {
    uint64_t coli = st[7U];
    col[7U] =
      coli
      ^
        ((coli & (uint64_t)0xeeeeeeeeeeeeeeeeU)
        >> (uint32_t)1U
        | (coli & (uint64_t)0x1111111111111111U) << (uint32_t)3U);
  }
  uint64_t col0 = col[0U];
  uint64_t
  ncol0 =
    col0
    ^
      ((col0 & (uint64_t)0xccccccccccccccccU)
      >> (uint32_t)2U
      | (col0 & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
  st[0U] = st[0U] ^ ncol0;
  {
    uint64_t prev = col[0U];
    uint64_t next = col[(uint32_t)0U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)0U + (uint32_t)1U] = st[(uint32_t)0U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  {
    uint64_t prev = col[1U];
    uint64_t next = col[(uint32_t)1U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)1U + (uint32_t)1U] = st[(uint32_t)1U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  {
    uint64_t prev = col[2U];
    uint64_t next = col[(uint32_t)2U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)2U + (uint32_t)1U] = st[(uint32_t)2U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  {
    uint64_t prev = col[3U];
    uint64_t next = col[(uint32_t)3U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)3U + (uint32_t)1U] = st[(uint32_t)3U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  {
    uint64_t prev = col[4U];
    uint64_t next = col[(uint32_t)4U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)4U + (uint32_t)1U] = st[(uint32_t)4U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  {
    uint64_t prev = col[5U];
    uint64_t next = col[(uint32_t)5U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)5U + (uint32_t)1U] = st[(uint32_t)5U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  {
    uint64_t prev = col[6U];
    uint64_t next = col[(uint32_t)6U + (uint32_t)1U];
    uint64_t
    ncoli =
      next
      ^
        ((next & (uint64_t)0xccccccccccccccccU)
        >> (uint32_t)2U
        | (next & (uint64_t)0x3333333333333333U) << (uint32_t)2U);
    st[(uint32_t)6U + (uint32_t)1U] = st[(uint32_t)6U + (uint32_t)1U] ^ (ncoli ^ prev);
  }
  st[0U] = st[0U] ^ col[7U];
  st[1U] = st[1U] ^ col[7U];
  st[3U] = st[3U] ^ col[7U];
  st[4U] = st[4U] ^ col[7U];
}

static void Hacl_Impl_AES_CoreBitSlice_aes_enc(uint64_t *st, uint64_t *key)
{
  Hacl_Impl_AES_CoreBitSlice_sub_bytes_state(st);
  Hacl_Impl_AES_CoreBitSlice_shift_rows_state(st);
  Hacl_Impl_AES_CoreBitSlice_mix_columns_state(st);
  Hacl_Impl_AES_CoreBitSlice_xor_state_key1(st, key);
}

static void Hacl_Impl_AES_CoreBitSlice_aes_enc_last(uint64_t *st, uint64_t *key)
{
  Hacl_Impl_AES_CoreBitSlice_sub_bytes_state(st);
  Hacl_Impl_AES_CoreBitSlice_shift_rows_state(st);
  Hacl_Impl_AES_CoreBitSlice_xor_state_key1(st, key);
}

static void
Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(uint64_t *next, uint64_t *prev, uint8_t rcon1)
{
  memcpy(next, prev, (uint32_t)8U * sizeof prev[0U]);
  Hacl_Impl_AES_CoreBitSlice_sub_bytes_state(next);
  {
    uint64_t u3 = next[0U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)0U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[0U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[1U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)1U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[1U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[2U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)2U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[2U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[3U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)3U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[3U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[4U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)4U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[4U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[5U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)5U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[5U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[6U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)6U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[6U] = n4 ^ u3 >> (uint32_t)4U;
  }
  {
    uint64_t u3 = next[7U] & (uint64_t)0xf000f000f000f000U;
    uint64_t n1 = u3 >> (uint32_t)12U;
    uint64_t n2 = (n1 >> (uint32_t)1U | n1 << (uint32_t)3U) & (uint64_t)0x000f000f000f000fU;
    uint64_t ri = (uint64_t)(rcon1 >> (uint32_t)7U & (uint8_t)1U);
    uint64_t ri1 = ri ^ ri << (uint32_t)16U;
    uint64_t ri2 = ri1 ^ ri1 << (uint32_t)32U;
    uint64_t n3 = n2 ^ ri2;
    uint64_t n4 = n3 << (uint32_t)12U;
    next[7U] = n4 ^ u3 >> (uint32_t)4U;
  }
}

static void Hacl_Impl_AES_CoreBitSlice_key_expansion_step(uint64_t *next, uint64_t *prev)
{
  {
    uint64_t p = prev[0U];
    uint64_t n1 = next[0U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[0U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[1U];
    uint64_t n1 = next[1U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[1U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[2U];
    uint64_t n1 = next[2U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[2U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[3U];
    uint64_t n1 = next[3U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[3U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[4U];
    uint64_t n1 = next[4U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[4U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[5U];
    uint64_t n1 = next[5U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[5U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[6U];
    uint64_t n1 = next[6U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[6U] = n1 ^ p1;
  }
  {
    uint64_t p = prev[7U];
    uint64_t n1 = next[7U];
    uint64_t
    p1 =
      p
      ^
        ((p & (uint64_t)0x0fff0fff0fff0fffU)
        << (uint32_t)4U
        ^
          ((p & (uint64_t)0x00ff00ff00ff00ffU)
          << (uint32_t)8U
          ^ (p & (uint64_t)0x000f000f000f000fU) << (uint32_t)12U));
    next[7U] = n1 ^ p1;
  }
}

static void
Hacl_Impl_AES_Generic_aes256_ctr_bitslice(
  uint32_t len,
  uint8_t *out,
  uint8_t *inp,
  uint64_t *ctx,
  uint32_t counter
)
{
  uint32_t blocks64 = len / (uint32_t)64U;
  for (uint32_t i = (uint32_t)0U; i < blocks64; i = i + (uint32_t)1U)
  {
    uint32_t ctr = counter + i * (uint32_t)4U;
    uint8_t *ib = inp + i * (uint32_t)64U;
    uint8_t *ob = out + i * (uint32_t)64U;
    uint64_t st[8U] = { 0U };
    uint64_t *kex = ctx + (uint32_t)8U;
    uint64_t *n1 = ctx;
    Hacl_Impl_AES_CoreBitSlice_load_state(st, n1, ctr);
    uint32_t klen1 = (uint32_t)8U;
    uint64_t *k0 = kex;
    uint64_t *kr = kex + klen1;
    uint64_t *kn = kex + (uint32_t)14U * klen1;
    Hacl_Impl_AES_CoreBitSlice_xor_state_key1(st, k0);
    for (uint32_t i = (uint32_t)0U; i < (uint32_t)13U; i = i + (uint32_t)1U)
    {
      uint64_t *sub_key = kr + i * (uint32_t)8U;
      Hacl_Impl_AES_CoreBitSlice_aes_enc(st, sub_key);
    }
    Hacl_Impl_AES_CoreBitSlice_aes_enc_last(st, kn);
    Hacl_Impl_AES_CoreBitSlice_xor_block(ob, st, ib);
  }
  uint32_t rem1 = len % (uint32_t)64U;
  uint8_t last1[64U];
  if (rem1 > (uint32_t)0U)
  {
    uint32_t ctr = counter + blocks64 * (uint32_t)4U;
    uint8_t *ib = inp + blocks64 * (uint32_t)64U;
    uint8_t *ob = out + blocks64 * (uint32_t)64U;
    uint8_t init = (uint8_t)0U;
    for (uint32_t i = (uint32_t)0U; i < (uint32_t)64U; i = i + (uint32_t)1U)
    {
      last1[i] = init;
    }
    memcpy(last1, ib, rem1 * sizeof ib[0U]);
    uint64_t st[8U] = { 0U };
    uint64_t *kex = ctx + (uint32_t)8U;
    uint64_t *n1 = ctx;
    Hacl_Impl_AES_CoreBitSlice_load_state(st, n1, ctr);
    uint32_t klen1 = (uint32_t)8U;
    uint64_t *k0 = kex;
    uint64_t *kr = kex + klen1;
    uint64_t *kn = kex + (uint32_t)14U * klen1;
    Hacl_Impl_AES_CoreBitSlice_xor_state_key1(st, k0);
    for (uint32_t i = (uint32_t)0U; i < (uint32_t)13U; i = i + (uint32_t)1U)
    {
      uint64_t *sub_key = kr + i * (uint32_t)8U;
      Hacl_Impl_AES_CoreBitSlice_aes_enc(st, sub_key);
    }
    Hacl_Impl_AES_CoreBitSlice_aes_enc_last(st, kn);
    Hacl_Impl_AES_CoreBitSlice_xor_block(last1, st, last1);
    memcpy(ob, last1, rem1 * sizeof last1[0U]);
  }
}

inline static void
Hacl_AES_256_BitSlice_aes256_init(uint64_t *ctx, uint8_t *key, uint8_t *nonce)
{
  uint64_t *kex = ctx + (uint32_t)8U;
  uint64_t *n1 = ctx;
  uint32_t klen1 = (uint32_t)8U;
  uint64_t *next0 = kex;
  uint64_t *next1 = kex + klen1;
  Hacl_Impl_AES_CoreBitSlice_load_key1(next0, key);
  Hacl_Impl_AES_CoreBitSlice_load_key1(next1, key + (uint32_t)16U);
  uint64_t *prev0 = next0;
  uint64_t *prev1 = next1;
  uint64_t *next01 = kex + klen1 * (uint32_t)2U;
  uint64_t *next11 = kex + klen1 * (uint32_t)3U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next01, prev1, (uint8_t)0x01U);
  {
    uint64_t n2 = next01[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[0U] = n5;
  }
  {
    uint64_t n2 = next01[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[1U] = n5;
  }
  {
    uint64_t n2 = next01[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[2U] = n5;
  }
  {
    uint64_t n2 = next01[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[3U] = n5;
  }
  {
    uint64_t n2 = next01[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[4U] = n5;
  }
  {
    uint64_t n2 = next01[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[5U] = n5;
  }
  {
    uint64_t n2 = next01[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[6U] = n5;
  }
  {
    uint64_t n2 = next01[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next01[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next01, prev0);
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next11, next01, (uint8_t)0U);
  {
    uint64_t n2 = next11[0U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[0U] = n5;
  }
  {
    uint64_t n2 = next11[1U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[1U] = n5;
  }
  {
    uint64_t n2 = next11[2U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[2U] = n5;
  }
  {
    uint64_t n2 = next11[3U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[3U] = n5;
  }
  {
    uint64_t n2 = next11[4U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[4U] = n5;
  }
  {
    uint64_t n2 = next11[5U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[5U] = n5;
  }
  {
    uint64_t n2 = next11[6U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[6U] = n5;
  }
  {
    uint64_t n2 = next11[7U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next11[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next11, prev1);
  uint64_t *prev01 = next01;
  uint64_t *prev11 = next11;
  uint64_t *next02 = kex + klen1 * (uint32_t)4U;
  uint64_t *next12 = kex + klen1 * (uint32_t)5U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next02, prev11, (uint8_t)0x02U);
  {
    uint64_t n2 = next02[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[0U] = n5;
  }
  {
    uint64_t n2 = next02[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[1U] = n5;
  }
  {
    uint64_t n2 = next02[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[2U] = n5;
  }
  {
    uint64_t n2 = next02[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[3U] = n5;
  }
  {
    uint64_t n2 = next02[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[4U] = n5;
  }
  {
    uint64_t n2 = next02[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[5U] = n5;
  }
  {
    uint64_t n2 = next02[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[6U] = n5;
  }
  {
    uint64_t n2 = next02[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next02[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next02, prev01);
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next12, next02, (uint8_t)0U);
  {
    uint64_t n2 = next12[0U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[0U] = n5;
  }
  {
    uint64_t n2 = next12[1U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[1U] = n5;
  }
  {
    uint64_t n2 = next12[2U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[2U] = n5;
  }
  {
    uint64_t n2 = next12[3U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[3U] = n5;
  }
  {
    uint64_t n2 = next12[4U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[4U] = n5;
  }
  {
    uint64_t n2 = next12[5U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[5U] = n5;
  }
  {
    uint64_t n2 = next12[6U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[6U] = n5;
  }
  {
    uint64_t n2 = next12[7U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next12[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next12, prev11);
  uint64_t *prev02 = next02;
  uint64_t *prev12 = next12;
  uint64_t *next03 = kex + klen1 * (uint32_t)6U;
  uint64_t *next13 = kex + klen1 * (uint32_t)7U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next03, prev12, (uint8_t)0x04U);
  {
    uint64_t n2 = next03[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[0U] = n5;
  }
  {
    uint64_t n2 = next03[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[1U] = n5;
  }
  {
    uint64_t n2 = next03[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[2U] = n5;
  }
  {
    uint64_t n2 = next03[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[3U] = n5;
  }
  {
    uint64_t n2 = next03[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[4U] = n5;
  }
  {
    uint64_t n2 = next03[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[5U] = n5;
  }
  {
    uint64_t n2 = next03[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[6U] = n5;
  }
  {
    uint64_t n2 = next03[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next03[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next03, prev02);
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next13, next03, (uint8_t)0U);
  {
    uint64_t n2 = next13[0U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[0U] = n5;
  }
  {
    uint64_t n2 = next13[1U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[1U] = n5;
  }
  {
    uint64_t n2 = next13[2U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[2U] = n5;
  }
  {
    uint64_t n2 = next13[3U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[3U] = n5;
  }
  {
    uint64_t n2 = next13[4U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[4U] = n5;
  }
  {
    uint64_t n2 = next13[5U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[5U] = n5;
  }
  {
    uint64_t n2 = next13[6U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[6U] = n5;
  }
  {
    uint64_t n2 = next13[7U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next13[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next13, prev12);
  uint64_t *prev03 = next03;
  uint64_t *prev13 = next13;
  uint64_t *next04 = kex + klen1 * (uint32_t)8U;
  uint64_t *next14 = kex + klen1 * (uint32_t)9U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next04, prev13, (uint8_t)0x08U);
  {
    uint64_t n2 = next04[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[0U] = n5;
  }
  {
    uint64_t n2 = next04[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[1U] = n5;
  }
  {
    uint64_t n2 = next04[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[2U] = n5;
  }
  {
    uint64_t n2 = next04[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[3U] = n5;
  }
  {
    uint64_t n2 = next04[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[4U] = n5;
  }
  {
    uint64_t n2 = next04[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[5U] = n5;
  }
  {
    uint64_t n2 = next04[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[6U] = n5;
  }
  {
    uint64_t n2 = next04[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next04[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next04, prev03);
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next14, next04, (uint8_t)0U);
  {
    uint64_t n2 = next14[0U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[0U] = n5;
  }
  {
    uint64_t n2 = next14[1U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[1U] = n5;
  }
  {
    uint64_t n2 = next14[2U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[2U] = n5;
  }
  {
    uint64_t n2 = next14[3U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[3U] = n5;
  }
  {
    uint64_t n2 = next14[4U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[4U] = n5;
  }
  {
    uint64_t n2 = next14[5U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[5U] = n5;
  }
  {
    uint64_t n2 = next14[6U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[6U] = n5;
  }
  {
    uint64_t n2 = next14[7U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next14[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next14, prev13);
  uint64_t *prev04 = next04;
  uint64_t *prev14 = next14;
  uint64_t *next05 = kex + klen1 * (uint32_t)10U;
  uint64_t *next15 = kex + klen1 * (uint32_t)11U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next05, prev14, (uint8_t)0x10U);
  {
    uint64_t n2 = next05[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[0U] = n5;
  }
  {
    uint64_t n2 = next05[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[1U] = n5;
  }
  {
    uint64_t n2 = next05[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[2U] = n5;
  }
  {
    uint64_t n2 = next05[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[3U] = n5;
  }
  {
    uint64_t n2 = next05[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[4U] = n5;
  }
  {
    uint64_t n2 = next05[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[5U] = n5;
  }
  {
    uint64_t n2 = next05[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[6U] = n5;
  }
  {
    uint64_t n2 = next05[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next05[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next05, prev04);
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next15, next05, (uint8_t)0U);
  {
    uint64_t n2 = next15[0U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[0U] = n5;
  }
  {
    uint64_t n2 = next15[1U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[1U] = n5;
  }
  {
    uint64_t n2 = next15[2U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[2U] = n5;
  }
  {
    uint64_t n2 = next15[3U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[3U] = n5;
  }
  {
    uint64_t n2 = next15[4U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[4U] = n5;
  }
  {
    uint64_t n2 = next15[5U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[5U] = n5;
  }
  {
    uint64_t n2 = next15[6U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[6U] = n5;
  }
  {
    uint64_t n2 = next15[7U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next15[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next15, prev14);
  uint64_t *prev05 = next05;
  uint64_t *prev15 = next15;
  uint64_t *next06 = kex + klen1 * (uint32_t)12U;
  uint64_t *next16 = kex + klen1 * (uint32_t)13U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next06, prev15, (uint8_t)0x20U);
  {
    uint64_t n2 = next06[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[0U] = n5;
  }
  {
    uint64_t n2 = next06[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[1U] = n5;
  }
  {
    uint64_t n2 = next06[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[2U] = n5;
  }
  {
    uint64_t n2 = next06[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[3U] = n5;
  }
  {
    uint64_t n2 = next06[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[4U] = n5;
  }
  {
    uint64_t n2 = next06[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[5U] = n5;
  }
  {
    uint64_t n2 = next06[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[6U] = n5;
  }
  {
    uint64_t n2 = next06[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next06[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next06, prev05);
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next16, next06, (uint8_t)0U);
  {
    uint64_t n2 = next16[0U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[0U] = n5;
  }
  {
    uint64_t n2 = next16[1U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[1U] = n5;
  }
  {
    uint64_t n2 = next16[2U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[2U] = n5;
  }
  {
    uint64_t n2 = next16[3U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[3U] = n5;
  }
  {
    uint64_t n2 = next16[4U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[4U] = n5;
  }
  {
    uint64_t n2 = next16[5U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[5U] = n5;
  }
  {
    uint64_t n2 = next16[6U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[6U] = n5;
  }
  {
    uint64_t n2 = next16[7U];
    uint64_t n3 = n2 & (uint64_t)0x0f000f000f000f00U;
    uint64_t n4 = n3 ^ n3 << (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next16[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next16, prev15);
  uint64_t *prev06 = next06;
  uint64_t *prev16 = next16;
  uint64_t *next07 = kex + klen1 * (uint32_t)14U;
  Hacl_Impl_AES_CoreBitSlice_aes_keygen_assist(next07, prev16, (uint8_t)0x40U);
  {
    uint64_t n2 = next07[0U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[0U] = n5;
  }
  {
    uint64_t n2 = next07[1U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[1U] = n5;
  }
  {
    uint64_t n2 = next07[2U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[2U] = n5;
  }
  {
    uint64_t n2 = next07[3U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[3U] = n5;
  }
  {
    uint64_t n2 = next07[4U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[4U] = n5;
  }
  {
    uint64_t n2 = next07[5U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[5U] = n5;
  }
  {
    uint64_t n2 = next07[6U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[6U] = n5;
  }
  {
    uint64_t n2 = next07[7U];
    uint64_t n3 = n2 & (uint64_t)0xf000f000f000f000U;
    uint64_t n4 = n3 ^ n3 >> (uint32_t)4U;
    uint64_t n5 = n4 ^ n4 >> (uint32_t)8U;
    next07[7U] = n5;
  }
  Hacl_Impl_AES_CoreBitSlice_key_expansion_step(next07, prev06);
  Hacl_Impl_AES_CoreBitSlice_load_nonce(n1, nonce);
}

inline static void Hacl_AES_256_BitSlice_aes256_set_nonce(uint64_t *ctx, uint8_t *nonce)
{
  uint64_t *n1 = ctx;
  Hacl_Impl_AES_CoreBitSlice_load_nonce(n1, nonce);
}

inline static void
Hacl_AES_256_BitSlice_aes256_key_block(uint8_t *kb, uint64_t *ctx, uint32_t counter)
{
  uint64_t *kex = ctx + (uint32_t)8U;
  uint64_t *n1 = ctx;
  uint64_t st[8U] = { 0U };
  Hacl_Impl_AES_CoreBitSlice_load_state(st, n1, counter);
  uint32_t klen1 = (uint32_t)8U;
  uint64_t *k0 = kex;
  uint64_t *kr = kex + klen1;
  uint64_t *kn = kex + (uint32_t)14U * klen1;
  Hacl_Impl_AES_CoreBitSlice_xor_state_key1(st, k0);
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)13U; i = i + (uint32_t)1U)
  {
    uint64_t *sub_key = kr + i * (uint32_t)8U;
    Hacl_Impl_AES_CoreBitSlice_aes_enc(st, sub_key);
  }
  Hacl_Impl_AES_CoreBitSlice_aes_enc_last(st, kn);
  Hacl_Impl_AES_CoreBitSlice_store_block0(kb, st);
}

static void
Hacl_AES_256_BitSlice_aes256_ctr(
  uint32_t len,
  uint8_t *out,
  uint8_t *inp,
  uint64_t *ctx,
  uint32_t c
)
{
  Hacl_Impl_AES_Generic_aes256_ctr_bitslice(len, out, inp, ctx, c);
}

static void Hacl_Impl_Gf128_FieldPreComp_prepare(uint64_t *pre, uint64_t *r)
{
  uint64_t sh[2U] = { 0U };
  sh[0U] = r[0U];
  sh[1U] = r[1U];
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)128U; i = i + (uint32_t)1U)
  {
    uint64_t s0 = sh[0U];
    uint64_t s1 = sh[1U];
    pre[i * (uint32_t)2U] = s0;
    pre[(uint32_t)1U + i * (uint32_t)2U] = s1;
    uint64_t m = (uint64_t)0U - (s0 & (uint64_t)1U);
    sh[0U] = s0 >> (uint32_t)1U | s1 << (uint32_t)63U;
    sh[1U] = s1 >> (uint32_t)1U ^ (m & (uint64_t)0xE100000000000000U);
  }
}

static void Hacl_Impl_Gf128_FieldPreComp_fmul(uint64_t *x, uint64_t *y)
{
  uint64_t tmp[2U] = { 0U };
  uint64_t sh[2U] = { 0U };
  sh[0U] = y[0U];
  sh[1U] = y[1U];
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)64U; i = i + (uint32_t)1U)
  {
    uint64_t s0 = sh[0U];
    uint64_t s1 = sh[1U];
    uint64_t m = (uint64_t)0U - (x[1U] >> ((uint32_t)63U - i) & (uint64_t)1U);
    tmp[0U] = tmp[0U] ^ (m & s0);
    tmp[1U] = tmp[1U] ^ (m & s1);
    uint64_t s = (uint64_t)0U - (s0 & (uint64_t)1U);
    sh[0U] = s0 >> (uint32_t)1U | s1 << (uint32_t)63U;
    sh[1U] = s1 >> (uint32_t)1U ^ (s & (uint64_t)0xE100000000000000U);
  }
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)64U; i = i + (uint32_t)1U)
  {
    uint64_t s0 = sh[0U];
    uint64_t s1 = sh[1U];
    uint64_t m = (uint64_t)0U - (x[0U] >> ((uint32_t)63U - i) & (uint64_t)1U);
    tmp[0U] = tmp[0U] ^ (m & s0);
    tmp[1U] = tmp[1U] ^ (m & s1);
    uint64_t s = (uint64_t)0U - (s0 & (uint64_t)1U);
    sh[0U] = s0 >> (uint32_t)1U | s1 << (uint32_t)63U;
    sh[1U] = s1 >> (uint32_t)1U ^ (s & (uint64_t)0xE100000000000000U);
  }
  x[0U] = tmp[0U];
  x[1U] = tmp[1U];
}

static void Hacl_Impl_Gf128_FieldPreComp_load_precompute_r(uint64_t *pre, uint8_t *key)
{
  uint64_t *r4 = pre;
  uint64_t *table = pre + (uint32_t)8U;
  uint64_t *r = r4 + (uint32_t)6U;
  uint64_t *r_2 = r4 + (uint32_t)4U;
  uint64_t *r_3 = r4 + (uint32_t)2U;
  uint64_t *r_4 = r4;
  uint64_t u = load64_be(key);
  r[1U] = u;
  uint64_t u0 = load64_be(key + (uint32_t)8U);
  r[0U] = u0;
  r_3[0U] = r[0U];
  r_3[1U] = r[1U];
  r_2[0U] = r[0U];
  r_2[1U] = r[1U];
  Hacl_Impl_Gf128_FieldPreComp_fmul(r_2, r);
  Hacl_Impl_Gf128_FieldPreComp_fmul(r_3, r_2);
  r_4[0U] = r_2[0U];
  r_4[1U] = r_2[1U];
  Hacl_Impl_Gf128_FieldPreComp_fmul(r_4, r_2);
  Hacl_Impl_Gf128_FieldPreComp_prepare(table, r_4);
}

static void Hacl_Impl_Gf128_FieldPreComp_fmul_pre(uint64_t *x, uint64_t *pre)
{
  uint64_t *tab = pre + (uint32_t)8U;
  uint64_t tmp[2U] = { 0U };
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)64U; i = i + (uint32_t)1U)
  {
    uint64_t m = (uint64_t)0U - (x[1U] >> ((uint32_t)63U - i) & (uint64_t)1U);
    tmp[0U] = tmp[0U] ^ (m & tab[i * (uint32_t)2U]);
    tmp[1U] = tmp[1U] ^ (m & tab[(uint32_t)1U + i * (uint32_t)2U]);
  }
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)64U; i = i + (uint32_t)1U)
  {
    uint64_t m = (uint64_t)0U - (x[0U] >> ((uint32_t)63U - i) & (uint64_t)1U);
    tmp[0U] = tmp[0U] ^ (m & tab[(uint32_t)128U + i * (uint32_t)2U]);
    tmp[1U] = tmp[1U] ^ (m & tab[(uint32_t)129U + i * (uint32_t)2U]);
  }
  x[0U] = tmp[0U];
  x[1U] = tmp[1U];
}

static void Hacl_Gf128_PreComp_gcm_init(uint64_t *ctx, uint8_t *key)
{
  uint64_t *acc = ctx;
  uint64_t *pre = ctx + (uint32_t)2U;
  acc[0U] = (uint64_t)0U;
  acc[1U] = (uint64_t)0U;
  Hacl_Impl_Gf128_FieldPreComp_load_precompute_r(pre, key);
}

uint32_t Hacl_AES_256_GCM_M32_aes_gcm_ctx_len = (uint32_t)396U;

void Hacl_AES_256_GCM_M32_aes256_gcm_init(uint64_t *ctx, uint8_t *key, uint8_t *nonce)
{
  uint8_t gcm_key[16U] = { 0U };
  uint8_t tag_mix[16U] = { 0U };
  uint8_t nonce0[12U] = { 0U };
  uint64_t *aes_ctx = ctx;
  uint64_t *gcm_ctx = ctx + (uint32_t)128U;
  Hacl_AES_256_BitSlice_aes256_init(aes_ctx, key, nonce0);
  Hacl_AES_256_BitSlice_aes256_key_block(gcm_key, aes_ctx, (uint32_t)0U);
  Hacl_AES_256_BitSlice_aes256_set_nonce(aes_ctx, nonce);
  Hacl_AES_256_BitSlice_aes256_key_block(tag_mix, aes_ctx, (uint32_t)1U);
  Hacl_Gf128_PreComp_gcm_init(gcm_ctx, gcm_key);
  uint64_t u = load64_le(tag_mix);
  ctx[394U] = u;
  uint64_t u0 = load64_le(tag_mix + (uint32_t)8U);
  ctx[395U] = u0;
}

void
Hacl_AES_256_GCM_M32_aes256_gcm_encrypt(
  uint64_t *ctx,
  uint32_t len,
  uint8_t *out,
  uint8_t *text,
  uint32_t aad_len,
  uint8_t *aad
)
{
  uint8_t tmp[16U] = { 0U };
  uint8_t *cip = out;
  uint64_t *aes_ctx = ctx;
  uint64_t *gcm_ctx = ctx + (uint32_t)128U;
  uint64_t *tag_mix = ctx + (uint32_t)394U;
  Hacl_AES_256_BitSlice_aes256_ctr(len, cip, text, aes_ctx, (uint32_t)2U);
  uint64_t b4[8U] = { 0U };
  uint64_t acc40[8U] = { 0U };
  uint64_t *acc5 = gcm_ctx;
  uint64_t *pre0 = gcm_ctx + (uint32_t)2U;
  memcpy(acc40, acc5, (uint32_t)2U * sizeof acc5[0U]);
  uint32_t blocks0 = aad_len / (uint32_t)64U;
  if (blocks0 > (uint32_t)0U)
  {
    uint8_t *tb = aad;
    uint64_t *x00 = b4;
    uint8_t *y00 = tb;
    uint64_t *x10 = b4 + (uint32_t)2U;
    uint8_t *y10 = tb + (uint32_t)16U;
    uint64_t *x20 = b4 + (uint32_t)4U;
    uint8_t *y20 = tb + (uint32_t)32U;
    uint64_t *x30 = b4 + (uint32_t)6U;
    uint8_t *y30 = tb + (uint32_t)48U;
    uint64_t u0 = load64_be(y00);
    x00[1U] = u0;
    uint64_t u1 = load64_be(y00 + (uint32_t)8U);
    x00[0U] = u1;
    uint64_t u2 = load64_be(y10);
    x10[1U] = u2;
    uint64_t u3 = load64_be(y10 + (uint32_t)8U);
    x10[0U] = u3;
    uint64_t u4 = load64_be(y20);
    x20[1U] = u4;
    uint64_t u5 = load64_be(y20 + (uint32_t)8U);
    x20[0U] = u5;
    uint64_t u6 = load64_be(y30);
    x30[1U] = u6;
    uint64_t u7 = load64_be(y30 + (uint32_t)8U);
    x30[0U] = u7;
    uint64_t *x01 = acc40;
    uint64_t *y01 = b4;
    uint64_t *x11 = acc40 + (uint32_t)2U;
    uint64_t *y11 = b4 + (uint32_t)2U;
    uint64_t *x21 = acc40 + (uint32_t)4U;
    uint64_t *y21 = b4 + (uint32_t)4U;
    uint64_t *x31 = acc40 + (uint32_t)6U;
    uint64_t *y31 = b4 + (uint32_t)6U;
    x01[0U] = x01[0U] ^ y01[0U];
    x01[1U] = x01[1U] ^ y01[1U];
    x11[0U] = x11[0U] ^ y11[0U];
    x11[1U] = x11[1U] ^ y11[1U];
    x21[0U] = x21[0U] ^ y21[0U];
    x21[1U] = x21[1U] ^ y21[1U];
    x31[0U] = x31[0U] ^ y31[0U];
    x31[1U] = x31[1U] ^ y31[1U];
    for (uint32_t i = (uint32_t)0U; i < blocks0 - (uint32_t)1U; i = i + (uint32_t)1U)
    {
      uint8_t *tb1 = aad + (i + (uint32_t)1U) * (uint32_t)64U;
      uint64_t *x0 = b4;
      uint8_t *y00 = tb1;
      uint64_t *x10 = b4 + (uint32_t)2U;
      uint8_t *y10 = tb1 + (uint32_t)16U;
      uint64_t *x20 = b4 + (uint32_t)4U;
      uint8_t *y20 = tb1 + (uint32_t)32U;
      uint64_t *x30 = b4 + (uint32_t)6U;
      uint8_t *y30 = tb1 + (uint32_t)48U;
      uint64_t u = load64_be(y00);
      x0[1U] = u;
      uint64_t u0 = load64_be(y00 + (uint32_t)8U);
      x0[0U] = u0;
      uint64_t u1 = load64_be(y10);
      x10[1U] = u1;
      uint64_t u2 = load64_be(y10 + (uint32_t)8U);
      x10[0U] = u2;
      uint64_t u3 = load64_be(y20);
      x20[1U] = u3;
      uint64_t u4 = load64_be(y20 + (uint32_t)8U);
      x20[0U] = u4;
      uint64_t u5 = load64_be(y30);
      x30[1U] = u5;
      uint64_t u6 = load64_be(y30 + (uint32_t)8U);
      x30[0U] = u6;
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40, pre0);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40 + (uint32_t)2U, pre0);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40 + (uint32_t)4U, pre0);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40 + (uint32_t)6U, pre0);
      uint64_t *x00 = acc40;
      uint64_t *y0 = b4;
      uint64_t *x1 = acc40 + (uint32_t)2U;
      uint64_t *y1 = b4 + (uint32_t)2U;
      uint64_t *x2 = acc40 + (uint32_t)4U;
      uint64_t *y2 = b4 + (uint32_t)4U;
      uint64_t *x3 = acc40 + (uint32_t)6U;
      uint64_t *y3 = b4 + (uint32_t)6U;
      x00[0U] = x00[0U] ^ y0[0U];
      x00[1U] = x00[1U] ^ y0[1U];
      x1[0U] = x1[0U] ^ y1[0U];
      x1[1U] = x1[1U] ^ y1[1U];
      x2[0U] = x2[0U] ^ y2[0U];
      x2[1U] = x2[1U] ^ y2[1U];
      x3[0U] = x3[0U] ^ y3[0U];
      x3[1U] = x3[1U] ^ y3[1U];
    }
    uint64_t *r4 = pre0;
    uint64_t *r3 = pre0 + (uint32_t)2U;
    uint64_t *r2 = pre0 + (uint32_t)4U;
    uint64_t *r = pre0 + (uint32_t)6U;
    uint64_t *acc0 = acc40;
    uint64_t *acc1 = acc40 + (uint32_t)2U;
    uint64_t *acc2 = acc40 + (uint32_t)4U;
    uint64_t *acc3 = acc40 + (uint32_t)6U;
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc0, r4);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r3);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc2, r2);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc3, r);
    memcpy(acc5, acc0, (uint32_t)2U * sizeof acc0[0U]);
    acc5[0U] = acc5[0U] ^ acc1[0U];
    acc5[1U] = acc5[1U] ^ acc1[1U];
    acc5[0U] = acc5[0U] ^ acc2[0U];
    acc5[1U] = acc5[1U] ^ acc2[1U];
    acc5[0U] = acc5[0U] ^ acc3[0U];
    acc5[1U] = acc5[1U] ^ acc3[1U];
  }
  uint32_t rem10 = aad_len % (uint32_t)64U;
  uint8_t *last10 = aad + blocks0 * (uint32_t)64U;
  uint64_t *acc10 = gcm_ctx;
  uint64_t *r0 = gcm_ctx + (uint32_t)8U;
  uint32_t blocks10 = rem10 / (uint32_t)16U;
  for (uint32_t i = (uint32_t)0U; i < blocks10; i = i + (uint32_t)1U)
  {
    uint8_t *tb = last10 + i * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint64_t u = load64_be(tb);
    elem[1U] = u;
    uint64_t u0 = load64_be(tb + (uint32_t)8U);
    elem[0U] = u0;
    acc10[0U] = acc10[0U] ^ elem[0U];
    acc10[1U] = acc10[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc10, r0);
  }
  uint32_t rem20 = rem10 % (uint32_t)16U;
  if (rem20 > (uint32_t)0U)
  {
    uint8_t *last2 = last10 + blocks10 * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint8_t b[16U] = { 0U };
    memcpy(b, last2, rem20 * sizeof last2[0U]);
    uint64_t u = load64_be(b);
    elem[1U] = u;
    uint64_t u0 = load64_be(b + (uint32_t)8U);
    elem[0U] = u0;
    acc10[0U] = acc10[0U] ^ elem[0U];
    acc10[1U] = acc10[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc10, r0);
  }
  uint64_t b40[8U] = { 0U };
  uint64_t acc41[8U] = { 0U };
  uint64_t *acc6 = gcm_ctx;
  uint64_t *pre1 = gcm_ctx + (uint32_t)2U;
  memcpy(acc41, acc6, (uint32_t)2U * sizeof acc6[0U]);
  uint32_t blocks2 = len / (uint32_t)64U;
  if (blocks2 > (uint32_t)0U)
  {
    uint8_t *tb = cip;
    uint64_t *x00 = b40;
    uint8_t *y00 = tb;
    uint64_t *x10 = b40 + (uint32_t)2U;
    uint8_t *y10 = tb + (uint32_t)16U;
    uint64_t *x20 = b40 + (uint32_t)4U;
    uint8_t *y20 = tb + (uint32_t)32U;
    uint64_t *x30 = b40 + (uint32_t)6U;
    uint8_t *y30 = tb + (uint32_t)48U;
    uint64_t u0 = load64_be(y00);
    x00[1U] = u0;
    uint64_t u1 = load64_be(y00 + (uint32_t)8U);
    x00[0U] = u1;
    uint64_t u2 = load64_be(y10);
    x10[1U] = u2;
    uint64_t u3 = load64_be(y10 + (uint32_t)8U);
    x10[0U] = u3;
    uint64_t u4 = load64_be(y20);
    x20[1U] = u4;
    uint64_t u5 = load64_be(y20 + (uint32_t)8U);
    x20[0U] = u5;
    uint64_t u6 = load64_be(y30);
    x30[1U] = u6;
    uint64_t u7 = load64_be(y30 + (uint32_t)8U);
    x30[0U] = u7;
    uint64_t *x01 = acc41;
    uint64_t *y01 = b40;
    uint64_t *x11 = acc41 + (uint32_t)2U;
    uint64_t *y11 = b40 + (uint32_t)2U;
    uint64_t *x21 = acc41 + (uint32_t)4U;
    uint64_t *y21 = b40 + (uint32_t)4U;
    uint64_t *x31 = acc41 + (uint32_t)6U;
    uint64_t *y31 = b40 + (uint32_t)6U;
    x01[0U] = x01[0U] ^ y01[0U];
    x01[1U] = x01[1U] ^ y01[1U];
    x11[0U] = x11[0U] ^ y11[0U];
    x11[1U] = x11[1U] ^ y11[1U];
    x21[0U] = x21[0U] ^ y21[0U];
    x21[1U] = x21[1U] ^ y21[1U];
    x31[0U] = x31[0U] ^ y31[0U];
    x31[1U] = x31[1U] ^ y31[1U];
    for (uint32_t i = (uint32_t)0U; i < blocks2 - (uint32_t)1U; i = i + (uint32_t)1U)
    {
      uint8_t *tb1 = cip + (i + (uint32_t)1U) * (uint32_t)64U;
      uint64_t *x0 = b40;
      uint8_t *y00 = tb1;
      uint64_t *x10 = b40 + (uint32_t)2U;
      uint8_t *y10 = tb1 + (uint32_t)16U;
      uint64_t *x20 = b40 + (uint32_t)4U;
      uint8_t *y20 = tb1 + (uint32_t)32U;
      uint64_t *x30 = b40 + (uint32_t)6U;
      uint8_t *y30 = tb1 + (uint32_t)48U;
      uint64_t u = load64_be(y00);
      x0[1U] = u;
      uint64_t u0 = load64_be(y00 + (uint32_t)8U);
      x0[0U] = u0;
      uint64_t u1 = load64_be(y10);
      x10[1U] = u1;
      uint64_t u2 = load64_be(y10 + (uint32_t)8U);
      x10[0U] = u2;
      uint64_t u3 = load64_be(y20);
      x20[1U] = u3;
      uint64_t u4 = load64_be(y20 + (uint32_t)8U);
      x20[0U] = u4;
      uint64_t u5 = load64_be(y30);
      x30[1U] = u5;
      uint64_t u6 = load64_be(y30 + (uint32_t)8U);
      x30[0U] = u6;
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41, pre1);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41 + (uint32_t)2U, pre1);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41 + (uint32_t)4U, pre1);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41 + (uint32_t)6U, pre1);
      uint64_t *x00 = acc41;
      uint64_t *y0 = b40;
      uint64_t *x1 = acc41 + (uint32_t)2U;
      uint64_t *y1 = b40 + (uint32_t)2U;
      uint64_t *x2 = acc41 + (uint32_t)4U;
      uint64_t *y2 = b40 + (uint32_t)4U;
      uint64_t *x3 = acc41 + (uint32_t)6U;
      uint64_t *y3 = b40 + (uint32_t)6U;
      x00[0U] = x00[0U] ^ y0[0U];
      x00[1U] = x00[1U] ^ y0[1U];
      x1[0U] = x1[0U] ^ y1[0U];
      x1[1U] = x1[1U] ^ y1[1U];
      x2[0U] = x2[0U] ^ y2[0U];
      x2[1U] = x2[1U] ^ y2[1U];
      x3[0U] = x3[0U] ^ y3[0U];
      x3[1U] = x3[1U] ^ y3[1U];
    }
    uint64_t *r4 = pre1;
    uint64_t *r3 = pre1 + (uint32_t)2U;
    uint64_t *r2 = pre1 + (uint32_t)4U;
    uint64_t *r = pre1 + (uint32_t)6U;
    uint64_t *acc0 = acc41;
    uint64_t *acc1 = acc41 + (uint32_t)2U;
    uint64_t *acc2 = acc41 + (uint32_t)4U;
    uint64_t *acc3 = acc41 + (uint32_t)6U;
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc0, r4);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r3);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc2, r2);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc3, r);
    memcpy(acc6, acc0, (uint32_t)2U * sizeof acc0[0U]);
    acc6[0U] = acc6[0U] ^ acc1[0U];
    acc6[1U] = acc6[1U] ^ acc1[1U];
    acc6[0U] = acc6[0U] ^ acc2[0U];
    acc6[1U] = acc6[1U] ^ acc2[1U];
    acc6[0U] = acc6[0U] ^ acc3[0U];
    acc6[1U] = acc6[1U] ^ acc3[1U];
  }
  uint32_t rem11 = len % (uint32_t)64U;
  uint8_t *last11 = cip + blocks2 * (uint32_t)64U;
  uint64_t *acc11 = gcm_ctx;
  uint64_t *r1 = gcm_ctx + (uint32_t)8U;
  uint32_t blocks11 = rem11 / (uint32_t)16U;
  for (uint32_t i = (uint32_t)0U; i < blocks11; i = i + (uint32_t)1U)
  {
    uint8_t *tb = last11 + i * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint64_t u = load64_be(tb);
    elem[1U] = u;
    uint64_t u0 = load64_be(tb + (uint32_t)8U);
    elem[0U] = u0;
    acc11[0U] = acc11[0U] ^ elem[0U];
    acc11[1U] = acc11[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc11, r1);
  }
  uint32_t rem21 = rem11 % (uint32_t)16U;
  if (rem21 > (uint32_t)0U)
  {
    uint8_t *last2 = last11 + blocks11 * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint8_t b[16U] = { 0U };
    memcpy(b, last2, rem21 * sizeof last2[0U]);
    uint64_t u = load64_be(b);
    elem[1U] = u;
    uint64_t u0 = load64_be(b + (uint32_t)8U);
    elem[0U] = u0;
    acc11[0U] = acc11[0U] ^ elem[0U];
    acc11[1U] = acc11[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc11, r1);
  }
  store64_be(tmp, (uint64_t)(aad_len * (uint32_t)8U));
  store64_be(tmp + (uint32_t)8U, (uint64_t)(len * (uint32_t)8U));
  uint64_t b41[8U] = { 0U };
  uint64_t acc4[8U] = { 0U };
  uint64_t *acc = gcm_ctx;
  uint64_t *pre = gcm_ctx + (uint32_t)2U;
  memcpy(acc4, acc, (uint32_t)2U * sizeof acc[0U]);
  uint32_t blocks = (uint32_t)0U;
  if (blocks > (uint32_t)0U)
  {
    uint8_t *tb = tmp;
    uint64_t *x00 = b41;
    uint8_t *y00 = tb;
    uint64_t *x10 = b41 + (uint32_t)2U;
    uint8_t *y10 = tb + (uint32_t)16U;
    uint64_t *x20 = b41 + (uint32_t)4U;
    uint8_t *y20 = tb + (uint32_t)32U;
    uint64_t *x30 = b41 + (uint32_t)6U;
    uint8_t *y30 = tb + (uint32_t)48U;
    uint64_t u0 = load64_be(y00);
    x00[1U] = u0;
    uint64_t u1 = load64_be(y00 + (uint32_t)8U);
    x00[0U] = u1;
    uint64_t u2 = load64_be(y10);
    x10[1U] = u2;
    uint64_t u3 = load64_be(y10 + (uint32_t)8U);
    x10[0U] = u3;
    uint64_t u4 = load64_be(y20);
    x20[1U] = u4;
    uint64_t u5 = load64_be(y20 + (uint32_t)8U);
    x20[0U] = u5;
    uint64_t u6 = load64_be(y30);
    x30[1U] = u6;
    uint64_t u7 = load64_be(y30 + (uint32_t)8U);
    x30[0U] = u7;
    uint64_t *x01 = acc4;
    uint64_t *y01 = b41;
    uint64_t *x11 = acc4 + (uint32_t)2U;
    uint64_t *y11 = b41 + (uint32_t)2U;
    uint64_t *x21 = acc4 + (uint32_t)4U;
    uint64_t *y21 = b41 + (uint32_t)4U;
    uint64_t *x31 = acc4 + (uint32_t)6U;
    uint64_t *y31 = b41 + (uint32_t)6U;
    x01[0U] = x01[0U] ^ y01[0U];
    x01[1U] = x01[1U] ^ y01[1U];
    x11[0U] = x11[0U] ^ y11[0U];
    x11[1U] = x11[1U] ^ y11[1U];
    x21[0U] = x21[0U] ^ y21[0U];
    x21[1U] = x21[1U] ^ y21[1U];
    x31[0U] = x31[0U] ^ y31[0U];
    x31[1U] = x31[1U] ^ y31[1U];
    for (uint32_t i = (uint32_t)0U; i < blocks - (uint32_t)1U; i = i + (uint32_t)1U)
    {
      uint8_t *tb1 = tmp + (i + (uint32_t)1U) * (uint32_t)64U;
      uint64_t *x0 = b41;
      uint8_t *y00 = tb1;
      uint64_t *x10 = b41 + (uint32_t)2U;
      uint8_t *y10 = tb1 + (uint32_t)16U;
      uint64_t *x20 = b41 + (uint32_t)4U;
      uint8_t *y20 = tb1 + (uint32_t)32U;
      uint64_t *x30 = b41 + (uint32_t)6U;
      uint8_t *y30 = tb1 + (uint32_t)48U;
      uint64_t u = load64_be(y00);
      x0[1U] = u;
      uint64_t u0 = load64_be(y00 + (uint32_t)8U);
      x0[0U] = u0;
      uint64_t u1 = load64_be(y10);
      x10[1U] = u1;
      uint64_t u2 = load64_be(y10 + (uint32_t)8U);
      x10[0U] = u2;
      uint64_t u3 = load64_be(y20);
      x20[1U] = u3;
      uint64_t u4 = load64_be(y20 + (uint32_t)8U);
      x20[0U] = u4;
      uint64_t u5 = load64_be(y30);
      x30[1U] = u5;
      uint64_t u6 = load64_be(y30 + (uint32_t)8U);
      x30[0U] = u6;
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4, pre);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4 + (uint32_t)2U, pre);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4 + (uint32_t)4U, pre);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4 + (uint32_t)6U, pre);
      uint64_t *x00 = acc4;
      uint64_t *y0 = b41;
      uint64_t *x1 = acc4 + (uint32_t)2U;
      uint64_t *y1 = b41 + (uint32_t)2U;
      uint64_t *x2 = acc4 + (uint32_t)4U;
      uint64_t *y2 = b41 + (uint32_t)4U;
      uint64_t *x3 = acc4 + (uint32_t)6U;
      uint64_t *y3 = b41 + (uint32_t)6U;
      x00[0U] = x00[0U] ^ y0[0U];
      x00[1U] = x00[1U] ^ y0[1U];
      x1[0U] = x1[0U] ^ y1[0U];
      x1[1U] = x1[1U] ^ y1[1U];
      x2[0U] = x2[0U] ^ y2[0U];
      x2[1U] = x2[1U] ^ y2[1U];
      x3[0U] = x3[0U] ^ y3[0U];
      x3[1U] = x3[1U] ^ y3[1U];
    }
    uint64_t *r4 = pre;
    uint64_t *r3 = pre + (uint32_t)2U;
    uint64_t *r2 = pre + (uint32_t)4U;
    uint64_t *r = pre + (uint32_t)6U;
    uint64_t *acc0 = acc4;
    uint64_t *acc1 = acc4 + (uint32_t)2U;
    uint64_t *acc2 = acc4 + (uint32_t)4U;
    uint64_t *acc3 = acc4 + (uint32_t)6U;
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc0, r4);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r3);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc2, r2);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc3, r);
    memcpy(acc, acc0, (uint32_t)2U * sizeof acc0[0U]);
    acc[0U] = acc[0U] ^ acc1[0U];
    acc[1U] = acc[1U] ^ acc1[1U];
    acc[0U] = acc[0U] ^ acc2[0U];
    acc[1U] = acc[1U] ^ acc2[1U];
    acc[0U] = acc[0U] ^ acc3[0U];
    acc[1U] = acc[1U] ^ acc3[1U];
  }
  uint32_t rem1 = (uint32_t)16U;
  uint8_t *last1 = tmp + blocks * (uint32_t)64U;
  uint64_t *acc1 = gcm_ctx;
  uint64_t *r = gcm_ctx + (uint32_t)8U;
  uint32_t blocks1 = rem1 / (uint32_t)16U;
  for (uint32_t i = (uint32_t)0U; i < blocks1; i = i + (uint32_t)1U)
  {
    uint8_t *tb = last1 + i * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint64_t u = load64_be(tb);
    elem[1U] = u;
    uint64_t u0 = load64_be(tb + (uint32_t)8U);
    elem[0U] = u0;
    acc1[0U] = acc1[0U] ^ elem[0U];
    acc1[1U] = acc1[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r);
  }
  uint32_t rem2 = rem1 % (uint32_t)16U;
  if (rem2 > (uint32_t)0U)
  {
    uint8_t *last2 = last1 + blocks1 * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint8_t b[16U] = { 0U };
    memcpy(b, last2, rem2 * sizeof last2[0U]);
    uint64_t u = load64_be(b);
    elem[1U] = u;
    uint64_t u0 = load64_be(b + (uint32_t)8U);
    elem[0U] = u0;
    acc1[0U] = acc1[0U] ^ elem[0U];
    acc1[1U] = acc1[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r);
  }
  uint64_t *acc0 = gcm_ctx;
  store64_be(tmp, acc0[1U]);
  store64_be(tmp + (uint32_t)8U, acc0[0U]);
  uint64_t u0 = load64_le(tmp);
  uint64_t tmp0 = u0;
  uint64_t u = load64_le(tmp + (uint32_t)8U);
  uint64_t tmp1 = u;
  uint64_t tmp01 = tmp0 ^ tag_mix[0U];
  uint64_t tmp11 = tmp1 ^ tag_mix[1U];
  store64_le(out + len, tmp01);
  store64_le(out + len + (uint32_t)8U, tmp11);
}

bool
Hacl_AES_256_GCM_M32_aes256_gcm_decrypt(
  uint64_t *ctx,
  uint32_t len,
  uint8_t *out,
  uint8_t *cipher,
  uint32_t aad_len,
  uint8_t *aad
)
{
  uint8_t scratch[18U] = { 0U };
  uint8_t *text = scratch;
  uint8_t *result = scratch + (uint32_t)17U;
  uint8_t *ciphertext = cipher;
  uint8_t *tag = cipher + len;
  uint64_t *aes_ctx = ctx;
  uint64_t *gcm_ctx = ctx + (uint32_t)128U;
  uint64_t *tag_mix = ctx + (uint32_t)394U;
  uint64_t b4[8U] = { 0U };
  uint64_t acc40[8U] = { 0U };
  uint64_t *acc5 = gcm_ctx;
  uint64_t *pre0 = gcm_ctx + (uint32_t)2U;
  memcpy(acc40, acc5, (uint32_t)2U * sizeof acc5[0U]);
  uint32_t blocks0 = aad_len / (uint32_t)64U;
  if (blocks0 > (uint32_t)0U)
  {
    uint8_t *tb = aad;
    uint64_t *x00 = b4;
    uint8_t *y00 = tb;
    uint64_t *x10 = b4 + (uint32_t)2U;
    uint8_t *y10 = tb + (uint32_t)16U;
    uint64_t *x20 = b4 + (uint32_t)4U;
    uint8_t *y20 = tb + (uint32_t)32U;
    uint64_t *x30 = b4 + (uint32_t)6U;
    uint8_t *y30 = tb + (uint32_t)48U;
    uint64_t u0 = load64_be(y00);
    x00[1U] = u0;
    uint64_t u1 = load64_be(y00 + (uint32_t)8U);
    x00[0U] = u1;
    uint64_t u2 = load64_be(y10);
    x10[1U] = u2;
    uint64_t u3 = load64_be(y10 + (uint32_t)8U);
    x10[0U] = u3;
    uint64_t u4 = load64_be(y20);
    x20[1U] = u4;
    uint64_t u5 = load64_be(y20 + (uint32_t)8U);
    x20[0U] = u5;
    uint64_t u6 = load64_be(y30);
    x30[1U] = u6;
    uint64_t u7 = load64_be(y30 + (uint32_t)8U);
    x30[0U] = u7;
    uint64_t *x01 = acc40;
    uint64_t *y01 = b4;
    uint64_t *x11 = acc40 + (uint32_t)2U;
    uint64_t *y11 = b4 + (uint32_t)2U;
    uint64_t *x21 = acc40 + (uint32_t)4U;
    uint64_t *y21 = b4 + (uint32_t)4U;
    uint64_t *x31 = acc40 + (uint32_t)6U;
    uint64_t *y31 = b4 + (uint32_t)6U;
    x01[0U] = x01[0U] ^ y01[0U];
    x01[1U] = x01[1U] ^ y01[1U];
    x11[0U] = x11[0U] ^ y11[0U];
    x11[1U] = x11[1U] ^ y11[1U];
    x21[0U] = x21[0U] ^ y21[0U];
    x21[1U] = x21[1U] ^ y21[1U];
    x31[0U] = x31[0U] ^ y31[0U];
    x31[1U] = x31[1U] ^ y31[1U];
    for (uint32_t i = (uint32_t)0U; i < blocks0 - (uint32_t)1U; i = i + (uint32_t)1U)
    {
      uint8_t *tb1 = aad + (i + (uint32_t)1U) * (uint32_t)64U;
      uint64_t *x0 = b4;
      uint8_t *y00 = tb1;
      uint64_t *x10 = b4 + (uint32_t)2U;
      uint8_t *y10 = tb1 + (uint32_t)16U;
      uint64_t *x20 = b4 + (uint32_t)4U;
      uint8_t *y20 = tb1 + (uint32_t)32U;
      uint64_t *x30 = b4 + (uint32_t)6U;
      uint8_t *y30 = tb1 + (uint32_t)48U;
      uint64_t u = load64_be(y00);
      x0[1U] = u;
      uint64_t u0 = load64_be(y00 + (uint32_t)8U);
      x0[0U] = u0;
      uint64_t u1 = load64_be(y10);
      x10[1U] = u1;
      uint64_t u2 = load64_be(y10 + (uint32_t)8U);
      x10[0U] = u2;
      uint64_t u3 = load64_be(y20);
      x20[1U] = u3;
      uint64_t u4 = load64_be(y20 + (uint32_t)8U);
      x20[0U] = u4;
      uint64_t u5 = load64_be(y30);
      x30[1U] = u5;
      uint64_t u6 = load64_be(y30 + (uint32_t)8U);
      x30[0U] = u6;
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40, pre0);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40 + (uint32_t)2U, pre0);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40 + (uint32_t)4U, pre0);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc40 + (uint32_t)6U, pre0);
      uint64_t *x00 = acc40;
      uint64_t *y0 = b4;
      uint64_t *x1 = acc40 + (uint32_t)2U;
      uint64_t *y1 = b4 + (uint32_t)2U;
      uint64_t *x2 = acc40 + (uint32_t)4U;
      uint64_t *y2 = b4 + (uint32_t)4U;
      uint64_t *x3 = acc40 + (uint32_t)6U;
      uint64_t *y3 = b4 + (uint32_t)6U;
      x00[0U] = x00[0U] ^ y0[0U];
      x00[1U] = x00[1U] ^ y0[1U];
      x1[0U] = x1[0U] ^ y1[0U];
      x1[1U] = x1[1U] ^ y1[1U];
      x2[0U] = x2[0U] ^ y2[0U];
      x2[1U] = x2[1U] ^ y2[1U];
      x3[0U] = x3[0U] ^ y3[0U];
      x3[1U] = x3[1U] ^ y3[1U];
    }
    uint64_t *r4 = pre0;
    uint64_t *r3 = pre0 + (uint32_t)2U;
    uint64_t *r2 = pre0 + (uint32_t)4U;
    uint64_t *r = pre0 + (uint32_t)6U;
    uint64_t *acc0 = acc40;
    uint64_t *acc1 = acc40 + (uint32_t)2U;
    uint64_t *acc2 = acc40 + (uint32_t)4U;
    uint64_t *acc3 = acc40 + (uint32_t)6U;
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc0, r4);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r3);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc2, r2);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc3, r);
    memcpy(acc5, acc0, (uint32_t)2U * sizeof acc0[0U]);
    acc5[0U] = acc5[0U] ^ acc1[0U];
    acc5[1U] = acc5[1U] ^ acc1[1U];
    acc5[0U] = acc5[0U] ^ acc2[0U];
    acc5[1U] = acc5[1U] ^ acc2[1U];
    acc5[0U] = acc5[0U] ^ acc3[0U];
    acc5[1U] = acc5[1U] ^ acc3[1U];
  }
  uint32_t rem10 = aad_len % (uint32_t)64U;
  uint8_t *last10 = aad + blocks0 * (uint32_t)64U;
  uint64_t *acc10 = gcm_ctx;
  uint64_t *r0 = gcm_ctx + (uint32_t)8U;
  uint32_t blocks10 = rem10 / (uint32_t)16U;
  for (uint32_t i = (uint32_t)0U; i < blocks10; i = i + (uint32_t)1U)
  {
    uint8_t *tb = last10 + i * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint64_t u = load64_be(tb);
    elem[1U] = u;
    uint64_t u0 = load64_be(tb + (uint32_t)8U);
    elem[0U] = u0;
    acc10[0U] = acc10[0U] ^ elem[0U];
    acc10[1U] = acc10[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc10, r0);
  }
  uint32_t rem20 = rem10 % (uint32_t)16U;
  if (rem20 > (uint32_t)0U)
  {
    uint8_t *last2 = last10 + blocks10 * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint8_t b[16U] = { 0U };
    memcpy(b, last2, rem20 * sizeof last2[0U]);
    uint64_t u = load64_be(b);
    elem[1U] = u;
    uint64_t u0 = load64_be(b + (uint32_t)8U);
    elem[0U] = u0;
    acc10[0U] = acc10[0U] ^ elem[0U];
    acc10[1U] = acc10[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc10, r0);
  }
  uint64_t b40[8U] = { 0U };
  uint64_t acc41[8U] = { 0U };
  uint64_t *acc6 = gcm_ctx;
  uint64_t *pre1 = gcm_ctx + (uint32_t)2U;
  memcpy(acc41, acc6, (uint32_t)2U * sizeof acc6[0U]);
  uint32_t blocks2 = len / (uint32_t)64U;
  if (blocks2 > (uint32_t)0U)
  {
    uint8_t *tb = ciphertext;
    uint64_t *x00 = b40;
    uint8_t *y00 = tb;
    uint64_t *x10 = b40 + (uint32_t)2U;
    uint8_t *y10 = tb + (uint32_t)16U;
    uint64_t *x20 = b40 + (uint32_t)4U;
    uint8_t *y20 = tb + (uint32_t)32U;
    uint64_t *x30 = b40 + (uint32_t)6U;
    uint8_t *y30 = tb + (uint32_t)48U;
    uint64_t u0 = load64_be(y00);
    x00[1U] = u0;
    uint64_t u1 = load64_be(y00 + (uint32_t)8U);
    x00[0U] = u1;
    uint64_t u2 = load64_be(y10);
    x10[1U] = u2;
    uint64_t u3 = load64_be(y10 + (uint32_t)8U);
    x10[0U] = u3;
    uint64_t u4 = load64_be(y20);
    x20[1U] = u4;
    uint64_t u5 = load64_be(y20 + (uint32_t)8U);
    x20[0U] = u5;
    uint64_t u6 = load64_be(y30);
    x30[1U] = u6;
    uint64_t u7 = load64_be(y30 + (uint32_t)8U);
    x30[0U] = u7;
    uint64_t *x01 = acc41;
    uint64_t *y01 = b40;
    uint64_t *x11 = acc41 + (uint32_t)2U;
    uint64_t *y11 = b40 + (uint32_t)2U;
    uint64_t *x21 = acc41 + (uint32_t)4U;
    uint64_t *y21 = b40 + (uint32_t)4U;
    uint64_t *x31 = acc41 + (uint32_t)6U;
    uint64_t *y31 = b40 + (uint32_t)6U;
    x01[0U] = x01[0U] ^ y01[0U];
    x01[1U] = x01[1U] ^ y01[1U];
    x11[0U] = x11[0U] ^ y11[0U];
    x11[1U] = x11[1U] ^ y11[1U];
    x21[0U] = x21[0U] ^ y21[0U];
    x21[1U] = x21[1U] ^ y21[1U];
    x31[0U] = x31[0U] ^ y31[0U];
    x31[1U] = x31[1U] ^ y31[1U];
    for (uint32_t i = (uint32_t)0U; i < blocks2 - (uint32_t)1U; i = i + (uint32_t)1U)
    {
      uint8_t *tb1 = ciphertext + (i + (uint32_t)1U) * (uint32_t)64U;
      uint64_t *x0 = b40;
      uint8_t *y00 = tb1;
      uint64_t *x10 = b40 + (uint32_t)2U;
      uint8_t *y10 = tb1 + (uint32_t)16U;
      uint64_t *x20 = b40 + (uint32_t)4U;
      uint8_t *y20 = tb1 + (uint32_t)32U;
      uint64_t *x30 = b40 + (uint32_t)6U;
      uint8_t *y30 = tb1 + (uint32_t)48U;
      uint64_t u = load64_be(y00);
      x0[1U] = u;
      uint64_t u0 = load64_be(y00 + (uint32_t)8U);
      x0[0U] = u0;
      uint64_t u1 = load64_be(y10);
      x10[1U] = u1;
      uint64_t u2 = load64_be(y10 + (uint32_t)8U);
      x10[0U] = u2;
      uint64_t u3 = load64_be(y20);
      x20[1U] = u3;
      uint64_t u4 = load64_be(y20 + (uint32_t)8U);
      x20[0U] = u4;
      uint64_t u5 = load64_be(y30);
      x30[1U] = u5;
      uint64_t u6 = load64_be(y30 + (uint32_t)8U);
      x30[0U] = u6;
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41, pre1);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41 + (uint32_t)2U, pre1);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41 + (uint32_t)4U, pre1);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc41 + (uint32_t)6U, pre1);
      uint64_t *x00 = acc41;
      uint64_t *y0 = b40;
      uint64_t *x1 = acc41 + (uint32_t)2U;
      uint64_t *y1 = b40 + (uint32_t)2U;
      uint64_t *x2 = acc41 + (uint32_t)4U;
      uint64_t *y2 = b40 + (uint32_t)4U;
      uint64_t *x3 = acc41 + (uint32_t)6U;
      uint64_t *y3 = b40 + (uint32_t)6U;
      x00[0U] = x00[0U] ^ y0[0U];
      x00[1U] = x00[1U] ^ y0[1U];
      x1[0U] = x1[0U] ^ y1[0U];
      x1[1U] = x1[1U] ^ y1[1U];
      x2[0U] = x2[0U] ^ y2[0U];
      x2[1U] = x2[1U] ^ y2[1U];
      x3[0U] = x3[0U] ^ y3[0U];
      x3[1U] = x3[1U] ^ y3[1U];
    }
    uint64_t *r4 = pre1;
    uint64_t *r3 = pre1 + (uint32_t)2U;
    uint64_t *r2 = pre1 + (uint32_t)4U;
    uint64_t *r = pre1 + (uint32_t)6U;
    uint64_t *acc0 = acc41;
    uint64_t *acc1 = acc41 + (uint32_t)2U;
    uint64_t *acc2 = acc41 + (uint32_t)4U;
    uint64_t *acc3 = acc41 + (uint32_t)6U;
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc0, r4);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r3);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc2, r2);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc3, r);
    memcpy(acc6, acc0, (uint32_t)2U * sizeof acc0[0U]);
    acc6[0U] = acc6[0U] ^ acc1[0U];
    acc6[1U] = acc6[1U] ^ acc1[1U];
    acc6[0U] = acc6[0U] ^ acc2[0U];
    acc6[1U] = acc6[1U] ^ acc2[1U];
    acc6[0U] = acc6[0U] ^ acc3[0U];
    acc6[1U] = acc6[1U] ^ acc3[1U];
  }
  uint32_t rem11 = len % (uint32_t)64U;
  uint8_t *last11 = ciphertext + blocks2 * (uint32_t)64U;
  uint64_t *acc11 = gcm_ctx;
  uint64_t *r1 = gcm_ctx + (uint32_t)8U;
  uint32_t blocks11 = rem11 / (uint32_t)16U;
  for (uint32_t i = (uint32_t)0U; i < blocks11; i = i + (uint32_t)1U)
  {
    uint8_t *tb = last11 + i * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint64_t u = load64_be(tb);
    elem[1U] = u;
    uint64_t u0 = load64_be(tb + (uint32_t)8U);
    elem[0U] = u0;
    acc11[0U] = acc11[0U] ^ elem[0U];
    acc11[1U] = acc11[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc11, r1);
  }
  uint32_t rem21 = rem11 % (uint32_t)16U;
  if (rem21 > (uint32_t)0U)
  {
    uint8_t *last2 = last11 + blocks11 * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint8_t b[16U] = { 0U };
    memcpy(b, last2, rem21 * sizeof last2[0U]);
    uint64_t u = load64_be(b);
    elem[1U] = u;
    uint64_t u0 = load64_be(b + (uint32_t)8U);
    elem[0U] = u0;
    acc11[0U] = acc11[0U] ^ elem[0U];
    acc11[1U] = acc11[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc11, r1);
  }
  store64_be(text, (uint64_t)(aad_len * (uint32_t)8U));
  store64_be(text + (uint32_t)8U, (uint64_t)(len * (uint32_t)8U));
  uint64_t b41[8U] = { 0U };
  uint64_t acc4[8U] = { 0U };
  uint64_t *acc = gcm_ctx;
  uint64_t *pre = gcm_ctx + (uint32_t)2U;
  memcpy(acc4, acc, (uint32_t)2U * sizeof acc[0U]);
  uint32_t blocks = (uint32_t)0U;
  if (blocks > (uint32_t)0U)
  {
    uint8_t *tb = text;
    uint64_t *x00 = b41;
    uint8_t *y00 = tb;
    uint64_t *x10 = b41 + (uint32_t)2U;
    uint8_t *y10 = tb + (uint32_t)16U;
    uint64_t *x20 = b41 + (uint32_t)4U;
    uint8_t *y20 = tb + (uint32_t)32U;
    uint64_t *x30 = b41 + (uint32_t)6U;
    uint8_t *y30 = tb + (uint32_t)48U;
    uint64_t u0 = load64_be(y00);
    x00[1U] = u0;
    uint64_t u1 = load64_be(y00 + (uint32_t)8U);
    x00[0U] = u1;
    uint64_t u2 = load64_be(y10);
    x10[1U] = u2;
    uint64_t u3 = load64_be(y10 + (uint32_t)8U);
    x10[0U] = u3;
    uint64_t u4 = load64_be(y20);
    x20[1U] = u4;
    uint64_t u5 = load64_be(y20 + (uint32_t)8U);
    x20[0U] = u5;
    uint64_t u6 = load64_be(y30);
    x30[1U] = u6;
    uint64_t u7 = load64_be(y30 + (uint32_t)8U);
    x30[0U] = u7;
    uint64_t *x01 = acc4;
    uint64_t *y01 = b41;
    uint64_t *x11 = acc4 + (uint32_t)2U;
    uint64_t *y11 = b41 + (uint32_t)2U;
    uint64_t *x21 = acc4 + (uint32_t)4U;
    uint64_t *y21 = b41 + (uint32_t)4U;
    uint64_t *x31 = acc4 + (uint32_t)6U;
    uint64_t *y31 = b41 + (uint32_t)6U;
    x01[0U] = x01[0U] ^ y01[0U];
    x01[1U] = x01[1U] ^ y01[1U];
    x11[0U] = x11[0U] ^ y11[0U];
    x11[1U] = x11[1U] ^ y11[1U];
    x21[0U] = x21[0U] ^ y21[0U];
    x21[1U] = x21[1U] ^ y21[1U];
    x31[0U] = x31[0U] ^ y31[0U];
    x31[1U] = x31[1U] ^ y31[1U];
    for (uint32_t i = (uint32_t)0U; i < blocks - (uint32_t)1U; i = i + (uint32_t)1U)
    {
      uint8_t *tb1 = text + (i + (uint32_t)1U) * (uint32_t)64U;
      uint64_t *x0 = b41;
      uint8_t *y00 = tb1;
      uint64_t *x10 = b41 + (uint32_t)2U;
      uint8_t *y10 = tb1 + (uint32_t)16U;
      uint64_t *x20 = b41 + (uint32_t)4U;
      uint8_t *y20 = tb1 + (uint32_t)32U;
      uint64_t *x30 = b41 + (uint32_t)6U;
      uint8_t *y30 = tb1 + (uint32_t)48U;
      uint64_t u = load64_be(y00);
      x0[1U] = u;
      uint64_t u0 = load64_be(y00 + (uint32_t)8U);
      x0[0U] = u0;
      uint64_t u1 = load64_be(y10);
      x10[1U] = u1;
      uint64_t u2 = load64_be(y10 + (uint32_t)8U);
      x10[0U] = u2;
      uint64_t u3 = load64_be(y20);
      x20[1U] = u3;
      uint64_t u4 = load64_be(y20 + (uint32_t)8U);
      x20[0U] = u4;
      uint64_t u5 = load64_be(y30);
      x30[1U] = u5;
      uint64_t u6 = load64_be(y30 + (uint32_t)8U);
      x30[0U] = u6;
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4, pre);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4 + (uint32_t)2U, pre);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4 + (uint32_t)4U, pre);
      Hacl_Impl_Gf128_FieldPreComp_fmul_pre(acc4 + (uint32_t)6U, pre);
      uint64_t *x00 = acc4;
      uint64_t *y0 = b41;
      uint64_t *x1 = acc4 + (uint32_t)2U;
      uint64_t *y1 = b41 + (uint32_t)2U;
      uint64_t *x2 = acc4 + (uint32_t)4U;
      uint64_t *y2 = b41 + (uint32_t)4U;
      uint64_t *x3 = acc4 + (uint32_t)6U;
      uint64_t *y3 = b41 + (uint32_t)6U;
      x00[0U] = x00[0U] ^ y0[0U];
      x00[1U] = x00[1U] ^ y0[1U];
      x1[0U] = x1[0U] ^ y1[0U];
      x1[1U] = x1[1U] ^ y1[1U];
      x2[0U] = x2[0U] ^ y2[0U];
      x2[1U] = x2[1U] ^ y2[1U];
      x3[0U] = x3[0U] ^ y3[0U];
      x3[1U] = x3[1U] ^ y3[1U];
    }
    uint64_t *r4 = pre;
    uint64_t *r3 = pre + (uint32_t)2U;
    uint64_t *r2 = pre + (uint32_t)4U;
    uint64_t *r = pre + (uint32_t)6U;
    uint64_t *acc0 = acc4;
    uint64_t *acc1 = acc4 + (uint32_t)2U;
    uint64_t *acc2 = acc4 + (uint32_t)4U;
    uint64_t *acc3 = acc4 + (uint32_t)6U;
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc0, r4);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r3);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc2, r2);
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc3, r);
    memcpy(acc, acc0, (uint32_t)2U * sizeof acc0[0U]);
    acc[0U] = acc[0U] ^ acc1[0U];
    acc[1U] = acc[1U] ^ acc1[1U];
    acc[0U] = acc[0U] ^ acc2[0U];
    acc[1U] = acc[1U] ^ acc2[1U];
    acc[0U] = acc[0U] ^ acc3[0U];
    acc[1U] = acc[1U] ^ acc3[1U];
  }
  uint32_t rem1 = (uint32_t)16U;
  uint8_t *last1 = text + blocks * (uint32_t)64U;
  uint64_t *acc1 = gcm_ctx;
  uint64_t *r2 = gcm_ctx + (uint32_t)8U;
  uint32_t blocks1 = rem1 / (uint32_t)16U;
  for (uint32_t i = (uint32_t)0U; i < blocks1; i = i + (uint32_t)1U)
  {
    uint8_t *tb = last1 + i * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint64_t u = load64_be(tb);
    elem[1U] = u;
    uint64_t u0 = load64_be(tb + (uint32_t)8U);
    elem[0U] = u0;
    acc1[0U] = acc1[0U] ^ elem[0U];
    acc1[1U] = acc1[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r2);
  }
  uint32_t rem2 = rem1 % (uint32_t)16U;
  if (rem2 > (uint32_t)0U)
  {
    uint8_t *last2 = last1 + blocks1 * (uint32_t)16U;
    uint64_t elem[2U] = { 0U };
    uint8_t b[16U] = { 0U };
    memcpy(b, last2, rem2 * sizeof last2[0U]);
    uint64_t u = load64_be(b);
    elem[1U] = u;
    uint64_t u0 = load64_be(b + (uint32_t)8U);
    elem[0U] = u0;
    acc1[0U] = acc1[0U] ^ elem[0U];
    acc1[1U] = acc1[1U] ^ elem[1U];
    Hacl_Impl_Gf128_FieldPreComp_fmul(acc1, r2);
  }
  uint64_t *acc0 = gcm_ctx;
  store64_be(text, acc0[1U]);
  store64_be(text + (uint32_t)8U, acc0[0U]);
  uint64_t u0 = load64_le(text);
  uint64_t text0 = u0;
  uint64_t u = load64_le(text + (uint32_t)8U);
  uint64_t text1 = u;
  uint64_t text01 = text0 ^ tag_mix[0U];
  uint64_t text11 = text1 ^ tag_mix[1U];
  store64_le(text, text01);
  store64_le(text + (uint32_t)8U, text11);
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)16U; i = i + (uint32_t)1U)
  {
    result[0U] = result[0U] | (text[i] ^ tag[i]);
  }
  uint8_t res8 = result[0U];
  bool r;
  if (res8 == (uint8_t)0U)
  {
    Hacl_AES_256_BitSlice_aes256_ctr(len, out, ciphertext, aes_ctx, (uint32_t)2U);
    r = true;
  }
  else
  {
    r = false;
  }
  return r;
}

